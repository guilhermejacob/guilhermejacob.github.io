---
title: 'Inequality, Poverty Mapping, Small Area Estimation, and Random Forests: Brazilian municipalities in 2010'
author: ~
date: '2017-11-13'
slug: inequality-poverty-mapping-sae-random-forests-brazil-2010
categories: [ "R", "Complex Survey" , "Official Statistics" ]
tags: [ "Poverty Mapping" , "Rstats" , "Brazil" , "Inequality" , "DataViz" ]
bibliography: [sae-post.bib]
# link-citations: true
# math: true
---

After a (somewhat long) break, I came back for a new post. Yey!
This time, we're not discussing educational stuff, but another big theme: inequality in Brazil.
What happens when we combine 2010 Brazilian Census microdata, Small Area Estimation and Random Forests? Interesting, right?

## 2010 Brazilian Census

Well, it's not really the 2010 Census, but the sample. Maybe this needs further explanation. Every ten years, IBGE^[**Instituto Brasileiro de Geografia e Estat√≠stica** (i.e., "Brazilian Institute for Geography and Statistics").] conducts this enormous survey, collecting information about all Brazilian households. Yet, there are two questionnaires: universal and sample. The sample one is longer and only applies for 10% of the households. So, yes: sample questionnaire is for a sample of the census. Or this is how I think it works, anyway.

However, it is a truly massive sample: a dataset with over 17 million observations, with a complex survey design.

If you want to work with this dataset, you ~~definitely~~ should take a look at [this chapter of asdfree.com](http://asdfree.com/brazilian-censo-demografico-censo.html), a courtesy of Djalma Pessoa and Anthony D'Amico.

## Small area estimation

**"Since this is a large sample, all estimates have a small variance, right?"** Well, that depends. For national level estimates, that is ~~almost always~~ true. For subpopulations, that might not be the case. 

Take municipalities, for instance. Inequality estimates from large cities often show smaller variance than those of small towns. The plot below shows inequality estimates and their 95% confidence intervals for the two tails of the index distribution.

```{r cv_plot , echo=FALSE, message=FALSE, warning=FALSE , cache = FALSE }
library(survey)
load( file.path( "~/GitLab/src-website/static/datasets" , "BR2010_PovEsts.Rdata" ) )
gini_data <- data.frame( 
  codmun7 = as.character(gini_list[,1]) , 
  gini_est = coef(gini_list) ,
  gini_var = (SE(gini_list))^2 ,
  ci_l = confint(gini_list)[ , 1 ] , 
  ci_u = confint(gini_list)[ , 2 ] , 
  gini_cv = cv( gini_list ) , 
  stringsAsFactors = FALSE )


library(ggplot2)
library(gridExtra)
library(grid)
library(scales)
library(ggiraph)

gini_data <- gini_data[ order( -gini_data$gini_est ) , ]
gini_data$codmun7 <- factor( gini_data$codmun7 , levels = unique( gini_data$codmun7 ) )

dist_plot <- ggplot( rbind( head( gini_data , 20 ) , tail( gini_data , 20 ) ) , aes( codmun7, gini_est , color = gini_cv ) ) +
  geom_point( stat = "identity" , size = .5 ) +
  geom_errorbar( aes( ymin=ci_l, ymax=ci_u ) , width=.1 , alpha = .5 ) +
  # geom_histogram( stat = "identity" , color = "transparent" ) + 
  scale_y_continuous( expand = c(0,0) , breaks = seq( 0, .9, .1 ) , labels = sprintf("%0.3f", seq( 0, .9, .1 ) ) ) +
  coord_cartesian(ylim=c( 0 , 1) ) +
  scale_color_continuous( low = "blue" , high = "red" ) +
  labs( x = "Municipalities" , y = "Gini Index" , 
        title = "Gini index estimates and 95% CIs" ,
        subtitle ="40 municipalities of with the largest or lowest estimates." , 
        caption = "Source: Brazil 2010 Census Sample. Microdata." ) +
  guides( fill = FALSE , color = FALSE ) +
  theme_dark() +
  theme( axis.text.x=element_blank(), axis.ticks.x=element_blank() , 
         panel.background = element_rect( fill = "white" ) , 
         panel.grid.minor.y = element_blank() ,
         panel.grid.minor.x = element_blank() ,
         panel.grid.major.x = element_blank() , 
         plot.title = element_text(lineheight=.8, face="bold") ) 

gini_data <- gini_data[ order( -gini_data$gini_cv ) , ]
gini_data$codmun7 <- factor( gini_data$codmun7 , levels = unique( gini_data$codmun7 ) )

# nobs <- 100
# ggplot( rbind( head( gini_data , nobs ) , tail( gini_data , nobs ) ), aes( codmun7, gini_cv , fill = gini_cv ) ) +
cv_plot <- ggplot( gini_data , aes( codmun7, gini_cv , fill = gini_cv ) ) +
  geom_bar( stat = "identity" , color = "transparent" ) +
  # geom_histogram( stat = "identity" , color = "transparent" ) + 
  scale_y_continuous( labels = scales::percent , expand = c(0,0) ) +
  coord_cartesian(ylim=c(0 , .275) ) +
  scale_fill_continuous( low = "blue" , high = "red" ) +
  labs( x = "Municipalities" , y = "Coefficient of Variation" , 
        title = "Gini Index" ,
        subtitle ="For each 5565 municipalities in the Census." , 
        caption = "Source: Brazil 2010 Census Sample. Microdata." ) +
  theme_dark() +
  theme( axis.text.x=element_blank(), axis.ticks.x=element_blank() , 
         panel.background = element_rect( fill = "white" ) , 
         panel.grid.minor.x = element_blank() ,
         panel.grid.major.x = element_blank() , 
         plot.title = element_text(lineheight=.8, face="bold") ) 

ggiraph( code = {print(dist_plot)} , zoom_max = 5 , width = 1 , tooltip_opacity = .7 , width_svg = 8, height_svg= 4 )

```

Some of those numbers have prohibitively large standard errors. Does that make them useless? Not at all! There is a branch of modern statistical techniques called *Small Area Estimation*^[This term and "small domain estimation" are synonyms. Nonetheless, the latter emphasizes the fact that such methods are not limited to areas in a spatial sense, but to subpopulations in general.], and it is **just beautiful**. There are several methods for doing it^[A nice survey of methods is given by @ghosh1994.], but the basic idea is to "borrow strength" from auxiliary data. Also, it can be seen as a bias-variance tradeoff: the direct estimates are unbiased but have higher variance, while the synthetic estimates are more precise but subjected to bias^[In this case, the terms "direct estimate" and "synthetic estimate" mean survey estimate and model estimate, respectively.].

One of the seminal techniques comes from @fay1979, where a model is proposed to correct survey estimates. The final estimates can be seen as convex linear combinations of the direct and synthetic estimates; i.e.,

$$ \hat{\theta}_i = \lambda_i \hat{y}^{DIR}_i + (1 - \lambda_i) \hat{y}^{SYN}_i, \\ \lambda_i \in [0,1], \hat{\lambda}_i = f(\hat{\sigma}^{DIR}_i , \hat{\sigma}^{SYN}_i) $$

where $\lambda$ is a parameter that "balances" the linear combinations. Given a function $f$ such that: 

$$ \lambda_i = f(\hat{\sigma}^{DIR}_i , \hat{\sigma}^{SYN}_i) = \frac{ \hat{\sigma}^{SYN}_i }{ \hat{\sigma}^{DIR}_i + \hat{\sigma}^{SYN}_i } \text{ ,}$$

this means that, as the direct estimate variance gets smaller, $\lambda$ gets closer to 1; hence, the "weight" assigned to the synthetic estimate diminishes. Conversely, when direct estimate variance gets larger, $\lambda$ gets closer to 0; hence, the "weight" assigned to the synthetic estimate increases. Clearly, we are simply balancing survey estimates and model estimates based on their precision!

## Poverty Mapping

Put simply, poverty mapping is an application of small area estimation to the spatial analysis of poverty and inequality. Since the publishing of now classic @elbers2003, given the recent advances in terms of data access and data mining techniques, this topic has gained more attention and, as a result, new books and articles are being published.

I should also mention that @pratesi2016 provides an excellent guide to the subject, including SAS and R codes.

## Random Forests

The main inspiration to this post is @bilton2017, where the application of classification trees for poverty mapping is explored. A similar (yet different!) idea with another set of data mining techniques is presented in @pluli2016. I must ~~respectfully~~ admit I envy not having those ideas.

> **Caution:** Since I'm writing a blog post and not a thesis, I will be very superficial in many aspects. For everything that I'm unable to explain throughly, I suggest the reader to take a look at the references in the end. That said, let's proceed.

Decision trees are a somewhat new technique for data analysis. Well, it has been around for 50 years [@loh2014], it is quite recent if we consider how long linear regression has been around... Decision trees is a general term that falls into two techniques: classification trees and regression trees. Put simply, classification trees are used to predict a discrete variable, while regression trees applies to a continuous variable.

The idea here is using objects' features to produce a set of rules that predicts which class/value a certain (predicted) variable will take. This set of rules is called a *decision tree*. If you, like myself, come from an econometrics background, that might look a bit weird. But don't worry too much: there are statistical tests and theories for that.

But what about Random Forests? A forest is lot of trees. A random forest is a bunch of random trees. So, I guess this explains it :)

Jokes aside, here's a simplified explanation of how the random forest algorithm works: 

1. Take a dataset and re-sample several times;
2. At each re-sample, choose a random set of features;
2. Given a re-sample and a random set of features, estimate a decision tree; 
3. Aggregate the set of estimated decision trees in order into get a single decision tree.

This procedure relates to the "bagging"^["Bagging" is a short term for "bootstrap aggregation".] technique, but the choice of a random set of features is the key behind the Random Forests algorithm. By doing this, we aggregate a set of uncorrelated decision trees.

~~In the end: *several trees $\rightarrow$ a forest*.~~

## In practice

First, we take the Gini index estimates and their variances for every municipality of the country. For this exercise, I used the ~~lovely~~ `convey` R package to estimate them [@pessoa2017]^[More info about this package and how to use them can be found [here](https://guilhermejacob.github.io/context/). Mas, se voc√™ prefere algo em portugu√™s, veja o cap√≠tulo 4 [deste livro](https://pt.scribd.com/document/361600824/Statistics-With-R-Inovacao-no-Mercado-Profissional).]. This is the same data used in the previous plot. Here's how it looks in a map.

![Map with the original Gini index estimates.](../../../img/gini_map_init.png)

The non-transparent areas have prohibitively large CVs for the estimate. Let's try to circumvent that. After training the Random Forest predictor, we get these results:

```{r train_rf, echo=FALSE, message=FALSE, warning=FALSE , cache = TRUE}
# load data
library(survey)
load( file.path( "~/GitLab/src-website/static/datasets" , "BR2010_PovEsts.Rdata" ) )
gini_data <- data.frame( 
  codmun7 = as.character(gini_list[,1]) , 
  gini_est = coef(gini_list) ,
  gini_var = (SE(gini_list))^2 ,
  ci_l = confint(gini_list)[ , 1 ] , 
  ci_u = confint(gini_list)[ , 2 ] , 
  gini_cv = cv( gini_list ) , 
  stringsAsFactors = FALSE )

# create high_cv identifier
gini_data$high_cv <- factor( 1*( gini_data$gini_cv > .05 ) , labels = c( "baixo" , "alto" ) )

# if IDHM dataset doesn't exist, download it:
idhm.mun_path <- file.path( "~/GitLab/src-website/static/datasets" , "MunIDHM.Rds" )
idhm.mun_path <- normalizePath( idhm.mun_path , mustWork = FALSE )
if ( !file.exists( idhm.mun_path ) ){
  tf <- tempfile()
  download.file( "http://www.atlasbrasil.org.br/2013/data/rawData/atlas2013_dadosbrutos_pt.xlsx" , tf )
  
  idhm.mun <- readxl::read_xlsx( tf , sheet = 2 )
  
  saveRDS( idhm.mun , file = idhm.mun_path , compress = TRUE )
  
} else {
  idhm.mun <- readRDS( idhm.mun_path )
}

# standardize column names
colnames(idhm.mun) <- tolower(colnames(idhm.mun))
colnames(idhm.mun) <- iconv( colnames(idhm.mun) , to = "ASCII" , sub = "" )
idhm.mun[, grep( "cod" , colnames(idhm.mun) ) ] <- apply( idhm.mun[, grep( "cod" , colnames(idhm.mun) ) ] , 2 , as.character )

# subset for related data
idhm.mun <- subset( idhm.mun , ano == 2010 )

# drop unused columns
idhm.mun <- idhm.mun[ , !grepl( "codmun6|munic|gini|idhm|peso|^mulh|^homem|^t_fund|^ren|trab|pob|^rdpc[0-9]|^esp" , colnames(idhm.mun) ) ]

# merge datasets
idhm.mun <- merge( idhm.mun , gini_data[ , c( "codmun7" , "gini_est" , "high_cv" , "gini_var" ) ] , by = "codmun7" )

# get high-cv obs
high_cv <- idhm.mun$high_cv
idhm.mun$high_cv <- NULL

# get saompling variance estimates
smp_var <- idhm.mun$gini_var
idhm.mun$gini_var <- NULL

# set seed of random processes
set.seed(123)

# prepare training and test dataset
library(caret)
inTrain <- createDataPartition( as.factor( idhm.mun$uf ) , p = .6 , list = FALSE )

# remove high_cv from inTrain
# inTrain <- inTrain[ inTrain %in% seq_along(high_cv)[ high_cv == "alto" ] ]

# train Random Forests
library(randomForest)
sae.rf <- randomForest(gini_est ~ ., data = idhm.mun[ inTrain, ], keep.inbag= TRUE , sampsize=100 , replace=TRUE , ntree=800 )

# predict
gini.result <- predict( sae.rf, idhm.mun[ -inTrain, ], type="response", predict.all=FALSE, proximity=FALSE )

# mean squared error
cat( "MSE:" , round( sqrt( mean( ( gini.result - idhm.mun[ -inTrain, "gini_est" ] )^2 ) ) , 5 ) , "\n" )

# median absolute deviation
cat( "MAD:" , round( median( abs( gini.result - idhm.mun[ -inTrain, "gini_est" ] ) ) , 5 ) , "\n" )

# get variance estimation
# library(RFinfer)
# var.results <- rfPredVar( sae.rf, idhm.mun[ inTrain, ] , pred.data = idhm.mun[ -inTrain, ],  CI = FALSE, tree.type = "rf", prog.bar = TRUE )
# sqrt(var.results$pred.ij.var)

library(randomForestCI)
rfm_var <- randomForestInfJack(sae.rf , idhm.mun , calibrate = TRUE)
# sqrt( rfm_var[ , 2]) / rfm_var[ , 1]

# balance estimates
gamma_est <- rfm_var[ , 2 ] / ( smp_var + rfm_var[ , 2 ] )

# get final estimates
gini_fh <- gamma_est*idhm.mun$gini_est + (1 - gamma_est) * rfm_var[ , 1 ]
gini_fh_var <- gamma_est * idhm.mun$gini_est + (1 - gamma_est)^2 * rfm_var[ , 2 ]

# save them to a dataset
gini_fh <- data.frame( codmun7 = idhm.mun$codmun7 , gini_smp = idhm.mun$gini_est , gini_rfm = rfm_var[ , 1 ] , gini_fh = gini_fh , gini_fh_var = gini_fh_var )

```

Those are the cross-validated results from the Random Forest model. It's quite good, apparently. Maybe too good. Still, here's the new map. 

<!-- <div style="width:15000px; height=20000px"> -->
<!-- ![Image](../../../img/gini_map_final.pdf) -->
<!-- </div> -->
![Map after the Random Forests-SAE procedure.](../../../img/gini_map_final.png)

You can see that differences have diminished a lot. That's probably a consequence of the bias-variance tradeoff. Finally, an open question I have after all this is: how do we calculate the variance of the final small area estimates? I'll think about that for a while.

Maybe I will come up with an answer. Maybe I won't.

*C'est la vie*.

## References